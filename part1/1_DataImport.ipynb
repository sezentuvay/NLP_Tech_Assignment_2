{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Data Import\n",
    "\n",
    "This script provides a function to read in the conll files and transfer them into a dataframe <br>\n",
    "\n",
    "\n",
    "*Input:*  \n",
    "- executionMode_dict\n",
    "- mode               -> ('production' / 'sample')\n",
    "- model              -> ('train' / 'test')\n",
    "- print_status       -> (True / False)\n",
    "- sentence_limit = None  (limit of sentences to import (default: None)\n",
    "\n",
    "*Output:* \n",
    "- executionMode_dict "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conll Description\n",
    "\n",
    "\"Sentences consist of one or more word lines, and word lines contain the following fields:\n",
    "\n",
    "ID: Word index, integer starting at 1 for each new sentence; may be a range for multiword tokens; may be a decimal number for empty nodes (decimal numbers can be lower than 1 but must be greater than 0). <br>\n",
    "FORM: Word form or punctuation symbol. <br>\n",
    "LEMMA: Lemma or stem of word form. <br>\n",
    "UPOS: Universal part-of-speech tag. <br>\n",
    "XPOS: Language-specific part-of-speech tag; underscore if not available. <br>\n",
    "FEATS: List of morphological features from the universal feature inventory or from a defined language-specific extension; underscore if not available. <br>\n",
    "HEAD: Head of the current word, which is either a value of ID or zero (0). <br>\n",
    "DEPREL: Universal dependency relation to the HEAD (root iff HEAD = 0) or a defined language-specific subtype of one. <br>\n",
    "DEPS: Enhanced dependency graph in the form of a list of head-deprel pairs. <br>\n",
    "MISC: Any other annotation.\n",
    "\n",
    "The fields DEPS and MISC replace the obsolete fields PHEAD and PDEPREL of the CoNLL-X format. In addition, we have modified the usage of the ID, FORM, LEMMA, XPOS, FEATS and HEAD fields as explained below.\n",
    "\n",
    "The fields must additionally meet the following constraints:\n",
    "\n",
    "Fields must not be empty.\n",
    "Fields other than FORM, LEMMA, and MISC must not contain space characters.\n",
    "Underscore (_) is used to denote unspecified values in all fields except ID. Note that no format-level distinction is made for the rare cases where the FORM or LEMMA is the literal underscore â€“ processing in such cases is application-dependent. Further, in UD treebanks the UPOS, HEAD, and DEPREL columns are not allowed to be left unspecified except in multiword tokens, where all must be unspecified, and empty nodes, where UPOS is optional and HEAD and DEPREL must be unspecified. The enhanced DEPS annotation is optional in UD treebanks, but if it is provided, it must be provided for all sentences in the treebank. \"\n",
    "\n",
    "\n",
    "*** taken from https://universaldependencies.org/format.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieved header according to documentation\n",
    "conll_header = ['id', 'form', 'lemma', 'upos', 'xpos', 'feats', 'head', 'deprel', 'deps', 'misc']\n",
    "\n",
    "# header from lecture form 25.02.\n",
    "conll_header = ['id', 'form', 'lemma', 'upos', 'xpos', 'morph', 'head', 'dep', 'head_dep', 'space', 'predicate', 'label']\n",
    "\n",
    "# included sentenceId\n",
    "conll_header_adapted = ['sentenceId', 'id', 'form', 'lemma', 'upos', 'xpos', 'morph', 'head', 'dep', 'head_dep', 'space', 'predicate', 'label']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "\n",
    "### Util Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve longest line\n",
    "# -> required for the creation of the dataframe later\n",
    "def retrieveLength(path_to_file):\n",
    "    c = 0\n",
    "    max_line_length = -1\n",
    "    sentences = 0\n",
    "    tokens = 0\n",
    "    with open(path_to_file) as file:\n",
    "        for line in file:\n",
    "\n",
    "\n",
    "            if line.startswith('# text'):\n",
    "                sentences += 1\n",
    "            elif line.startswith('#') or line.startswith('\\n'):\n",
    "                pass\n",
    "            else:\n",
    "                values = line.split('\\t')\n",
    "                line_length = len(values)\n",
    "                if line_length > max_line_length:\n",
    "                    max_line_length = line_length\n",
    "\n",
    "                tokens += 1\n",
    "\n",
    "            c += 1   \n",
    "    \n",
    "    return max_line_length, tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversion into dataframe\n",
    "def createDataFrame(executionMode_dict,\n",
    "                    mode,                   #('production' / 'sample')\n",
    "                    model,                  #('train' / 'test')\n",
    "                    print_status   = False,\n",
    "                    sentence_limit = None):\n",
    "\n",
    "    \n",
    "    # variable assignments\n",
    "    path_to_file = executionMode_dict[mode]['input'][model]\n",
    "    path_to_save = '../data/intermediate/' + mode + '_' + model +'_01_importedData.csv'\n",
    "    \n",
    "    \n",
    "    executionMode_dict[mode]['intermediate'][model] = {}\n",
    "    executionMode_dict[mode]['intermediate'][model]['1_imported'] = path_to_save\n",
    "    #executionMode_dict[mode]['intermediate'][model]['df']   = ''\n",
    "    \n",
    "    \n",
    "    # start retrieving\n",
    "    \n",
    "    max_line_length, tokens = retrieveLength(path_to_file)\n",
    "    sentences = -1\n",
    "\n",
    "    ### create header\n",
    "    \n",
    "    # create empty dataframe with known columns and fillers for remaining collumns\n",
    "    headers_df = np.full(max_line_length + 1, np.str)  #  + 1 to add sentence column\n",
    "    \n",
    "    # add sentence column to header\n",
    "    #headers_df[1] = \n",
    "    \n",
    "    # add columns from identified columns\n",
    "    headers_df[:len(conll_header_adapted)] = conll_header_adapted\n",
    "    \n",
    "    # fill remaining column headers with '_'\n",
    "    required_length_to_fill = len(headers_df) - len(conll_header_adapted)\n",
    "    label_headers = np.full(required_length_to_fill, 'label')\n",
    "    numbers_list  = np.arange(1, required_length_to_fill + 1)\n",
    "    numbers_list = np.array([str(n) for n in numbers_list])\n",
    "    label_headers = np.char.add(label_headers, numbers_list)\n",
    "    headers_df[len(conll_header_adapted):] = label_headers\n",
    "    \n",
    "    \n",
    "    ### create dataframe\n",
    "    df = pd.DataFrame(columns=headers_df)\n",
    "\n",
    "    \n",
    "    ### fill dataframe\n",
    "\n",
    "    # loop through file\n",
    "    with open(path_to_file) as file:\n",
    "        for line in file:\n",
    "\n",
    "            # pass all other lines\n",
    "            if line.startswith('# text'):\n",
    "                sentences += 1\n",
    "                \n",
    "            elif line.startswith('#') or line.startswith('\\n'):\n",
    "                pass\n",
    "            \n",
    "            # only go into token lines\n",
    "            else:\n",
    "                \n",
    "                # omit linebreaks from some lines\n",
    "                if line.endswith('\\n'):\n",
    "                    line = line.replace('\\n', '')\n",
    "                \n",
    "                # split input line\n",
    "                values = np.array(line.split('\\t'))\n",
    "\n",
    "                array  = np.full(max_line_length+1, np.str)\n",
    "                \n",
    "                # add sentenceId\n",
    "                array[0] = sentences\n",
    "                # add retrieved information from conll file\n",
    "                array[1:len(values)+1] = values\n",
    "                # fill remaining columns   !!** use np.nan ?! **!! \n",
    "                array[len(values)+1:] = '_'\n",
    "    \n",
    "                # create new entry\n",
    "                df_entry = pd.DataFrame(columns=headers_df, data=[array])\n",
    "\n",
    "                # concatenate to large dataframe\n",
    "                df = pd.concat([df, df_entry], axis = 0, ignore_index=True)\n",
    "\n",
    "            if type(sentence_limit) == int and sentences >= sentence_limit:\n",
    "                break\n",
    "                \n",
    "    if print_status == True:\n",
    "        \n",
    "        print('\\n\\n#### 1 Data Import ####\\n\\n')\n",
    "        \n",
    "        print(f' - # Sentences in file: {sentences}')\n",
    "        print(f' - # Tokens in file: {tokens}')\n",
    "        print(f' - Maxium of columns in file: {max_line_length}')\n",
    "        \n",
    "        print(f'\\n  - ## {len(df.sentenceId.unique())} sentences were added to dataframe.')\n",
    "        \n",
    "        print(f' - Dataframe saved under: {path_to_save}')\n",
    "        \n",
    "    \n",
    "    df.to_csv(path_to_save, index=False )\n",
    "    \n",
    "    return executionMode_dict\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
